{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on April 13 (with 2-hour grace period) and worth 25 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1. You will:\n",
    "\n",
    "1. Evaluate baseline models using default settings.\n",
    "2. Engineer new features and re-evaluate models.\n",
    "3. Use feature selection techniques to find promising subsets.\n",
    "4. Select the top 3 models and fine-tune them for optimal performance.\n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling.\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">   df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will not use the testing set during this milestone — it’s reserved for final evaluation later.\n",
    "- You will have to redo the scaling step when you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv(\"zillow_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = df.drop(columns='taxvaluedollarcnt')\n",
    "y = df['taxvaluedollarcnt']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>finishedsquarefeet12</th>\n",
       "      <th>fips</th>\n",
       "      <th>fireplacecnt</th>\n",
       "      <th>garagecarcnt</th>\n",
       "      <th>garagetotalsqft</th>\n",
       "      <th>lotsizesquarefeet</th>\n",
       "      <th>poolcnt</th>\n",
       "      <th>propertylandusetypeid</th>\n",
       "      <th>rawcensustractandblock</th>\n",
       "      <th>regionidcity</th>\n",
       "      <th>regionidcounty</th>\n",
       "      <th>regionidzip</th>\n",
       "      <th>roomcnt</th>\n",
       "      <th>numberofstories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75034</th>\n",
       "      <td>11010994</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2286.0</td>\n",
       "      <td>2286.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17199.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>6.037107e+07</td>\n",
       "      <td>12447.0</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>96368.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42599</th>\n",
       "      <td>11407829</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3430.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>6.037601e+07</td>\n",
       "      <td>45888.0</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>96133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71027</th>\n",
       "      <td>12302592</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1556.0</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3664.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>6.037536e+07</td>\n",
       "      <td>32616.0</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>96125.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62724</th>\n",
       "      <td>17290835</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>6.111007e+07</td>\n",
       "      <td>34278.0</td>\n",
       "      <td>2061.0</td>\n",
       "      <td>96385.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7790</th>\n",
       "      <td>14649487</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>6059.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>6.059110e+07</td>\n",
       "      <td>24832.0</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>97052.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       parcelid  bathroomcnt  bedroomcnt  calculatedfinishedsquarefeet  \\\n",
       "75034  11010994          3.0         4.0                        2286.0   \n",
       "42599  11407829          1.0         2.0                         864.0   \n",
       "71027  12302592          2.0         4.0                        1556.0   \n",
       "62724  17290835          2.5         3.0                        1949.0   \n",
       "7790   14649487          2.5         2.0                        1386.0   \n",
       "\n",
       "       finishedsquarefeet12    fips  fireplacecnt  garagecarcnt  \\\n",
       "75034                2286.0  6037.0           0.0           0.0   \n",
       "42599                 864.0  6037.0           0.0           0.0   \n",
       "71027                1515.0  6037.0           0.0           0.0   \n",
       "62724                1949.0  6111.0           1.0           2.0   \n",
       "7790                 1386.0  6059.0           0.0           0.0   \n",
       "\n",
       "       garagetotalsqft  lotsizesquarefeet  poolcnt  propertylandusetypeid  \\\n",
       "75034              0.0            17199.0      0.0                  261.0   \n",
       "42599              0.0             3430.0      0.0                  261.0   \n",
       "71027              0.0             3664.0      0.0                  246.0   \n",
       "62724              0.0             7200.0      0.0                  266.0   \n",
       "7790               0.0             7200.0      0.0                  266.0   \n",
       "\n",
       "       rawcensustractandblock  regionidcity  regionidcounty  regionidzip  \\\n",
       "75034            6.037107e+07       12447.0          3101.0      96368.0   \n",
       "42599            6.037601e+07       45888.0          3101.0      96133.0   \n",
       "71027            6.037536e+07       32616.0          3101.0      96125.0   \n",
       "62724            6.111007e+07       34278.0          2061.0      96385.0   \n",
       "7790             6.059110e+07       24832.0          1286.0      97052.0   \n",
       "\n",
       "       roomcnt  numberofstories  \n",
       "75034      0.0              0.0  \n",
       "42599      0.0              0.0  \n",
       "71027      0.0              0.0  \n",
       "62724      7.0              2.0  \n",
       "7790       6.0              1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>finishedsquarefeet12</th>\n",
       "      <th>fips</th>\n",
       "      <th>fireplacecnt</th>\n",
       "      <th>garagecarcnt</th>\n",
       "      <th>garagetotalsqft</th>\n",
       "      <th>lotsizesquarefeet</th>\n",
       "      <th>poolcnt</th>\n",
       "      <th>propertylandusetypeid</th>\n",
       "      <th>rawcensustractandblock</th>\n",
       "      <th>regionidcity</th>\n",
       "      <th>regionidcounty</th>\n",
       "      <th>regionidzip</th>\n",
       "      <th>roomcnt</th>\n",
       "      <th>numberofstories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75034</th>\n",
       "      <td>-0.582844</td>\n",
       "      <td>0.763780</td>\n",
       "      <td>0.849543</td>\n",
       "      <td>0.628465</td>\n",
       "      <td>0.697315</td>\n",
       "      <td>-0.569744</td>\n",
       "      <td>-0.316933</td>\n",
       "      <td>-0.654129</td>\n",
       "      <td>-0.519005</td>\n",
       "      <td>-0.087355</td>\n",
       "      <td>-0.507892</td>\n",
       "      <td>-0.158844</td>\n",
       "      <td>-0.583968</td>\n",
       "      <td>-0.441944</td>\n",
       "      <td>0.707220</td>\n",
       "      <td>-0.061669</td>\n",
       "      <td>-0.523241</td>\n",
       "      <td>-0.497515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42599</th>\n",
       "      <td>-0.466933</td>\n",
       "      <td>-1.343706</td>\n",
       "      <td>-0.922483</td>\n",
       "      <td>-1.043632</td>\n",
       "      <td>-1.044933</td>\n",
       "      <td>-0.569744</td>\n",
       "      <td>-0.316933</td>\n",
       "      <td>-0.654129</td>\n",
       "      <td>-0.519005</td>\n",
       "      <td>-0.202328</td>\n",
       "      <td>-0.507892</td>\n",
       "      <td>-0.158844</td>\n",
       "      <td>-0.559966</td>\n",
       "      <td>0.259868</td>\n",
       "      <td>0.707220</td>\n",
       "      <td>-0.129134</td>\n",
       "      <td>-0.523241</td>\n",
       "      <td>-0.497515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71027</th>\n",
       "      <td>-0.205584</td>\n",
       "      <td>-0.289963</td>\n",
       "      <td>0.849543</td>\n",
       "      <td>-0.229925</td>\n",
       "      <td>-0.247321</td>\n",
       "      <td>-0.569744</td>\n",
       "      <td>-0.316933</td>\n",
       "      <td>-0.654129</td>\n",
       "      <td>-0.519005</td>\n",
       "      <td>-0.200374</td>\n",
       "      <td>-0.507892</td>\n",
       "      <td>-3.014131</td>\n",
       "      <td>-0.563149</td>\n",
       "      <td>-0.018665</td>\n",
       "      <td>0.707220</td>\n",
       "      <td>-0.131431</td>\n",
       "      <td>-0.523241</td>\n",
       "      <td>-0.497515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62724</th>\n",
       "      <td>1.251421</td>\n",
       "      <td>0.236908</td>\n",
       "      <td>-0.036470</td>\n",
       "      <td>0.232194</td>\n",
       "      <td>0.284420</td>\n",
       "      <td>2.993375</td>\n",
       "      <td>2.206741</td>\n",
       "      <td>1.546033</td>\n",
       "      <td>-0.519005</td>\n",
       "      <td>-0.170848</td>\n",
       "      <td>-0.507892</td>\n",
       "      <td>0.792918</td>\n",
       "      <td>3.001574</td>\n",
       "      <td>0.016214</td>\n",
       "      <td>-0.590409</td>\n",
       "      <td>-0.056789</td>\n",
       "      <td>1.960004</td>\n",
       "      <td>2.556213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7790</th>\n",
       "      <td>0.479915</td>\n",
       "      <td>0.236908</td>\n",
       "      <td>-0.922483</td>\n",
       "      <td>-0.429825</td>\n",
       "      <td>-0.405373</td>\n",
       "      <td>0.489562</td>\n",
       "      <td>-0.316933</td>\n",
       "      <td>-0.654129</td>\n",
       "      <td>-0.519005</td>\n",
       "      <td>-0.170848</td>\n",
       "      <td>-0.507892</td>\n",
       "      <td>0.792918</td>\n",
       "      <td>0.483606</td>\n",
       "      <td>-0.182025</td>\n",
       "      <td>-1.557392</td>\n",
       "      <td>0.134698</td>\n",
       "      <td>1.605255</td>\n",
       "      <td>1.029349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37194</th>\n",
       "      <td>-0.566046</td>\n",
       "      <td>0.763780</td>\n",
       "      <td>0.849543</td>\n",
       "      <td>1.359860</td>\n",
       "      <td>1.459396</td>\n",
       "      <td>-0.569744</td>\n",
       "      <td>-0.316933</td>\n",
       "      <td>-0.654129</td>\n",
       "      <td>-0.519005</td>\n",
       "      <td>-0.145147</td>\n",
       "      <td>-0.507892</td>\n",
       "      <td>1.363975</td>\n",
       "      <td>-0.583886</td>\n",
       "      <td>-0.441944</td>\n",
       "      <td>0.707220</td>\n",
       "      <td>-0.065114</td>\n",
       "      <td>-0.523241</td>\n",
       "      <td>-0.497515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>1.186637</td>\n",
       "      <td>0.236908</td>\n",
       "      <td>0.849543</td>\n",
       "      <td>0.047581</td>\n",
       "      <td>0.092062</td>\n",
       "      <td>2.993375</td>\n",
       "      <td>4.730414</td>\n",
       "      <td>1.546033</td>\n",
       "      <td>1.666976</td>\n",
       "      <td>-0.178781</td>\n",
       "      <td>-0.507892</td>\n",
       "      <td>-0.158844</td>\n",
       "      <td>3.001278</td>\n",
       "      <td>0.138608</td>\n",
       "      <td>-0.590409</td>\n",
       "      <td>0.147904</td>\n",
       "      <td>1.605255</td>\n",
       "      <td>1.029349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54886</th>\n",
       "      <td>-0.156372</td>\n",
       "      <td>-0.289963</td>\n",
       "      <td>-0.036470</td>\n",
       "      <td>-0.010036</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>-0.569744</td>\n",
       "      <td>-0.316933</td>\n",
       "      <td>-0.654129</td>\n",
       "      <td>-0.519005</td>\n",
       "      <td>-0.188634</td>\n",
       "      <td>-0.507892</td>\n",
       "      <td>-0.158844</td>\n",
       "      <td>-0.561417</td>\n",
       "      <td>-0.445197</td>\n",
       "      <td>0.707220</td>\n",
       "      <td>-0.106455</td>\n",
       "      <td>-0.523241</td>\n",
       "      <td>-0.497515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-0.607221</td>\n",
       "      <td>-1.343706</td>\n",
       "      <td>-0.922483</td>\n",
       "      <td>-0.676759</td>\n",
       "      <td>-0.662667</td>\n",
       "      <td>-0.569744</td>\n",
       "      <td>-0.316933</td>\n",
       "      <td>-0.654129</td>\n",
       "      <td>-0.519005</td>\n",
       "      <td>-0.184760</td>\n",
       "      <td>-0.507892</td>\n",
       "      <td>-0.158844</td>\n",
       "      <td>-0.583105</td>\n",
       "      <td>-0.441944</td>\n",
       "      <td>0.707220</td>\n",
       "      <td>-0.039276</td>\n",
       "      <td>-0.523241</td>\n",
       "      <td>-0.497515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>-0.361382</td>\n",
       "      <td>-1.343706</td>\n",
       "      <td>0.849543</td>\n",
       "      <td>-0.599151</td>\n",
       "      <td>-0.581804</td>\n",
       "      <td>-0.569744</td>\n",
       "      <td>-0.316933</td>\n",
       "      <td>-0.654129</td>\n",
       "      <td>-0.519005</td>\n",
       "      <td>-0.188367</td>\n",
       "      <td>-0.507892</td>\n",
       "      <td>-0.158844</td>\n",
       "      <td>-0.578063</td>\n",
       "      <td>-0.441944</td>\n",
       "      <td>0.707220</td>\n",
       "      <td>-0.169613</td>\n",
       "      <td>-0.523241</td>\n",
       "      <td>-0.497515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61441 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       parcelid  bathroomcnt  bedroomcnt  calculatedfinishedsquarefeet  \\\n",
       "75034 -0.582844     0.763780    0.849543                      0.628465   \n",
       "42599 -0.466933    -1.343706   -0.922483                     -1.043632   \n",
       "71027 -0.205584    -0.289963    0.849543                     -0.229925   \n",
       "62724  1.251421     0.236908   -0.036470                      0.232194   \n",
       "7790   0.479915     0.236908   -0.922483                     -0.429825   \n",
       "...         ...          ...         ...                           ...   \n",
       "37194 -0.566046     0.763780    0.849543                      1.359860   \n",
       "6265   1.186637     0.236908    0.849543                      0.047581   \n",
       "54886 -0.156372    -0.289963   -0.036470                     -0.010036   \n",
       "860   -0.607221    -1.343706   -0.922483                     -0.676759   \n",
       "15795 -0.361382    -1.343706    0.849543                     -0.599151   \n",
       "\n",
       "       finishedsquarefeet12      fips  fireplacecnt  garagecarcnt  \\\n",
       "75034              0.697315 -0.569744     -0.316933     -0.654129   \n",
       "42599             -1.044933 -0.569744     -0.316933     -0.654129   \n",
       "71027             -0.247321 -0.569744     -0.316933     -0.654129   \n",
       "62724              0.284420  2.993375      2.206741      1.546033   \n",
       "7790              -0.405373  0.489562     -0.316933     -0.654129   \n",
       "...                     ...       ...           ...           ...   \n",
       "37194              1.459396 -0.569744     -0.316933     -0.654129   \n",
       "6265               0.092062  2.993375      4.730414      1.546033   \n",
       "54886              0.032027 -0.569744     -0.316933     -0.654129   \n",
       "860               -0.662667 -0.569744     -0.316933     -0.654129   \n",
       "15795             -0.581804 -0.569744     -0.316933     -0.654129   \n",
       "\n",
       "       garagetotalsqft  lotsizesquarefeet   poolcnt  propertylandusetypeid  \\\n",
       "75034        -0.519005          -0.087355 -0.507892              -0.158844   \n",
       "42599        -0.519005          -0.202328 -0.507892              -0.158844   \n",
       "71027        -0.519005          -0.200374 -0.507892              -3.014131   \n",
       "62724        -0.519005          -0.170848 -0.507892               0.792918   \n",
       "7790         -0.519005          -0.170848 -0.507892               0.792918   \n",
       "...                ...                ...       ...                    ...   \n",
       "37194        -0.519005          -0.145147 -0.507892               1.363975   \n",
       "6265          1.666976          -0.178781 -0.507892              -0.158844   \n",
       "54886        -0.519005          -0.188634 -0.507892              -0.158844   \n",
       "860          -0.519005          -0.184760 -0.507892              -0.158844   \n",
       "15795        -0.519005          -0.188367 -0.507892              -0.158844   \n",
       "\n",
       "       rawcensustractandblock  regionidcity  regionidcounty  regionidzip  \\\n",
       "75034               -0.583968     -0.441944        0.707220    -0.061669   \n",
       "42599               -0.559966      0.259868        0.707220    -0.129134   \n",
       "71027               -0.563149     -0.018665        0.707220    -0.131431   \n",
       "62724                3.001574      0.016214       -0.590409    -0.056789   \n",
       "7790                 0.483606     -0.182025       -1.557392     0.134698   \n",
       "...                       ...           ...             ...          ...   \n",
       "37194               -0.583886     -0.441944        0.707220    -0.065114   \n",
       "6265                 3.001278      0.138608       -0.590409     0.147904   \n",
       "54886               -0.561417     -0.445197        0.707220    -0.106455   \n",
       "860                 -0.583105     -0.441944        0.707220    -0.039276   \n",
       "15795               -0.578063     -0.441944        0.707220    -0.169613   \n",
       "\n",
       "        roomcnt  numberofstories  \n",
       "75034 -0.523241        -0.497515  \n",
       "42599 -0.523241        -0.497515  \n",
       "71027 -0.523241        -0.497515  \n",
       "62724  1.960004         2.556213  \n",
       "7790   1.605255         1.029349  \n",
       "...         ...              ...  \n",
       "37194 -0.523241        -0.497515  \n",
       "6265   1.605255         1.029349  \n",
       "54886 -0.523241        -0.497515  \n",
       "860   -0.523241        -0.497515  \n",
       "15795 -0.523241        -0.497515  \n",
       "\n",
       "[61441 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler **only on training data**\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_train_scaled\n",
    "\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Baseline Modeling [3 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters**:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each model:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV RMSE Score** across all folds in a table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE scorer (negative because sklearn expects higher = better)\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Bagging\": BaggingRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.138e+15, tolerance: 7.053e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.113e+15, tolerance: 6.955e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.115e+15, tolerance: 6.974e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.094e+15, tolerance: 6.930e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+15, tolerance: 7.056e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+15, tolerance: 7.047e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.120e+15, tolerance: 6.983e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+15, tolerance: 7.019e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.117e+15, tolerance: 6.972e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.103e+15, tolerance: 6.949e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.119e+15, tolerance: 6.986e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.112e+15, tolerance: 6.981e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.118e+15, tolerance: 7.002e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+15, tolerance: 7.002e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.116e+15, tolerance: 6.998e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.115e+15, tolerance: 6.989e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.120e+15, tolerance: 7.008e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+15, tolerance: 7.002e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+15, tolerance: 7.020e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.094e+15, tolerance: 6.950e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+15, tolerance: 7.005e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.104e+15, tolerance: 6.974e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.107e+15, tolerance: 6.966e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+15, tolerance: 7.042e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.113e+15, tolerance: 6.982e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Negative RMSE, so we multiply by -1 to get actual RMSE\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, scoring=rmse_scorer, cv=cv)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Mean RMSE\": np.mean(scores),\n",
    "        \"Std RMSE\": np.std(scores)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Mean RMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean RMSE</th>\n",
       "      <th>Std RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>-348268.020934</td>\n",
       "      <td>4642.080656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>-294780.081287</td>\n",
       "      <td>3092.177989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>-293445.942543</td>\n",
       "      <td>3087.588510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>-293276.018549</td>\n",
       "      <td>3082.936162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>-265893.554483</td>\n",
       "      <td>2591.974115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>-259283.419546</td>\n",
       "      <td>2760.647597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-255527.143875</td>\n",
       "      <td>2805.033850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model      Mean RMSE     Std RMSE\n",
       "3      Decision Tree -348268.020934  4642.080656\n",
       "2   Lasso Regression -294780.081287  3092.177989\n",
       "1   Ridge Regression -293445.942543  3087.588510\n",
       "0  Linear Regression -293276.018549  3082.936162\n",
       "4            Bagging -265893.554483  2591.974115\n",
       "6  Gradient Boosting -259283.419546  2760.647597\n",
       "5      Random Forest -255527.143875  2805.033850"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [2 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which models perform best overall?\n",
    "  - Which are most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest model performed the best overall, with the lowest average RMSE, meaning it made the most accurate predictions. It was also the most consistent, with the smallest variation in scores across different cross-validation folds. Gradient Boosting also performed well, coming in second in both accuracy and stability. In contrast, the Decision Tree had the highest RMSE and more variation, which could mean it's overfitting. The Linear, Ridge, and Lasso regression models all had similar and very stable results, but their higher RMSE scores suggest they may be underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [3 pts]\n",
    "\n",
    "Consider **at least three new features** based on your Milestone 1, Part 5. Examples include:\n",
    "- Polynomial terms\n",
    "- Log or interaction terms\n",
    "- Groupings or transformations of categorical features\n",
    "\n",
    "Add these features to `X_train` and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run all models listed above (using default settings again).\n",
    "- Report updated RMSE scores (mean and std) across repeated CV in a table. \n",
    "\n",
    "**Note:**  Recall that this will require creating a new version of the dataset, so effectively you may be running \"polynomial regression\" using `LinearRegression`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log of finishedsquarefeet12 (to reduce skew)\n",
    "X_train[\"log_finishedsqft\"] = np.log1p(X_train[\"finishedsquarefeet12\"])\n",
    "# Interaction term (bathroomcnt * bedroomcnt)\n",
    "X_train[\"bath_bed_interaction\"] = X_train[\"bathroomcnt\"] * X_train[\"bedroomcnt\"]\n",
    "# Ratio of finished sqft to lot size (space utilization)\n",
    "X_train[\"sqft_to_lot_ratio\"] = X_train[\"finishedsquarefeet12\"] / X_train[\"lotsizesquarefeet\"]\n",
    "\n",
    "# Drop the original column\n",
    "X_train = X_train.drop(columns=[\"finishedsquarefeet12\"])\n",
    "\n",
    "# Drop any inf or NaN values created by division\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "y_train = y_train.loc[X_train.index]  # align target with cleaned data\n",
    "\n",
    "# === Step 2: Standardize ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.152e+15, tolerance: 7.053e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+15, tolerance: 6.955e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+15, tolerance: 6.974e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.107e+15, tolerance: 6.930e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.139e+15, tolerance: 7.056e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.139e+15, tolerance: 7.047e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.132e+15, tolerance: 6.983e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.137e+15, tolerance: 7.019e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+15, tolerance: 6.972e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.116e+15, tolerance: 6.949e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+15, tolerance: 6.986e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+15, tolerance: 6.981e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.132e+15, tolerance: 7.002e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.135e+15, tolerance: 7.002e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+15, tolerance: 6.998e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+15, tolerance: 6.989e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+15, tolerance: 7.008e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.144e+15, tolerance: 7.002e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.141e+15, tolerance: 7.020e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.108e+15, tolerance: 6.950e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.143e+15, tolerance: 7.005e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.116e+15, tolerance: 6.974e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.121e+15, tolerance: 6.966e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.147e+15, tolerance: 7.042e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/timbeer/Documents/Python Projects/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+15, tolerance: 6.982e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, scoring=rmse_scorer, cv=cv)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Mean RMSE\": -np.mean(scores),  # Flip sign back to positive\n",
    "        \"Std RMSE\": np.std(scores)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Mean RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean RMSE</th>\n",
       "      <th>Std RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>255681.175268</td>\n",
       "      <td>2982.691378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>259154.727891</td>\n",
       "      <td>2856.577571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>266448.505889</td>\n",
       "      <td>3169.220836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>294333.329487</td>\n",
       "      <td>3031.977440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>294491.375769</td>\n",
       "      <td>3038.533044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>295732.922441</td>\n",
       "      <td>3046.856479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>350378.896721</td>\n",
       "      <td>4925.495184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model      Mean RMSE     Std RMSE\n",
       "5      Random Forest  255681.175268  2982.691378\n",
       "6  Gradient Boosting  259154.727891  2856.577571\n",
       "4            Bagging  266448.505889  3169.220836\n",
       "0  Linear Regression  294333.329487  3031.977440\n",
       "1   Ridge Regression  294491.375769  3038.533044\n",
       "2   Lasso Regression  295732.922441  3046.856479\n",
       "3      Decision Tree  350378.896721  4925.495184"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [2 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "- Were there any unexpected results?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding new features and dropping the original square footage column, most models showed little change in performance. Random Forest and Gradient Boosting still performed the best, with very similar RMSE scores as before. Bagging also stayed consistent. The linear models (Linear, Ridge, and Lasso) had small differences but remained less accurate than the tree-based models. The Decision Tree continued to perform the worst, though it became slightly more stable. Overall, the new features didn’t make a big difference, but they also didn’t hurt the models — showing that the feature engineering was safe to include.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible reason the new features didn’t help much is that the tree-based models, like Random Forest and Gradient Boosting, are already good at capturing interactions and nonlinear patterns. So adding things like the log transformation or interaction terms may not have added much new information for those models. For the linear models, the new features might have helped slightly by introducing nonlinearity, but they still struggled to match the performance of more flexible models.\n",
    "\n",
    "A bit of a surprise was that the Decision Tree model became more stable after adding the features, even though its overall accuracy stayed low. This suggests that the new features may have made the tree's splits a little more consistent across folds, even if they didn’t improve performance much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [3 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features.\n",
    "- Report updated RMSE scores (mean and std) across repeated CV in a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting features for: Linear Regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting features for: Ridge Regression\n",
      "Selecting features for: Lasso Regression\n",
      "Selecting features for: Decision Tree\n",
      "Selecting features for: Bagging\n",
      "Selecting features for: Random Forest\n",
      "Selecting features for: Gradient Boosting\n",
      "\n",
      "Linear Regression selected features (10):\n",
      "['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'garagetotalsqft', 'lotsizesquarefeet', 'poolcnt', 'propertylandusetypeid', 'regionidcounty', 'numberofstories', 'sqft_to_lot_ratio']\n",
      "\n",
      "Ridge Regression selected features (10):\n",
      "['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'garagetotalsqft', 'lotsizesquarefeet', 'poolcnt', 'propertylandusetypeid', 'regionidcounty', 'numberofstories', 'sqft_to_lot_ratio']\n",
      "\n",
      "Lasso Regression selected features (10):\n",
      "['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'garagetotalsqft', 'lotsizesquarefeet', 'poolcnt', 'propertylandusetypeid', 'regionidcounty', 'numberofstories', 'sqft_to_lot_ratio']\n",
      "\n",
      "Decision Tree selected features (10):\n",
      "['bathroomcnt', 'fips', 'fireplacecnt', 'garagecarcnt', 'propertylandusetypeid', 'rawcensustractandblock', 'regionidcity', 'regionidcounty', 'regionidzip', 'numberofstories']\n",
      "\n",
      "Bagging selected features (10):\n",
      "['parcelid', 'bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'garagetotalsqft', 'lotsizesquarefeet', 'poolcnt', 'rawcensustractandblock', 'regionidzip', 'log_finishedsqft']\n",
      "\n",
      "Random Forest selected features (10):\n",
      "['parcelid', 'bathroomcnt', 'lotsizesquarefeet', 'poolcnt', 'rawcensustractandblock', 'regionidcity', 'regionidzip', 'roomcnt', 'log_finishedsqft', 'bath_bed_interaction']\n",
      "\n",
      "Gradient Boosting selected features (10):\n",
      "['parcelid', 'bathroomcnt', 'bedroomcnt', 'poolcnt', 'rawcensustractandblock', 'regionidcity', 'regionidzip', 'roomcnt', 'log_finishedsqft', 'sqft_to_lot_ratio']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# Store selected features for each model\n",
    "selected_features_dict = {}\n",
    "\n",
    "# Base models dictionary\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Bagging\": BaggingRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Feature selection loop with fixed top 10\n",
    "for name, model in models.items():\n",
    "    print(f\"Selecting features for: {name}\")\n",
    "    \n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        # Tree-based: use top 10 from feature importances\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        importances = pd.Series(model.feature_importances_, index=X_train_scaled.columns)\n",
    "        top_features = importances.sort_values(ascending=False).head(10).index.tolist()\n",
    "    else:\n",
    "        # Other models: use SequentialFeatureSelector with n_features_to_select=10 (it takes too much time otherwise)\n",
    "        sfs = SequentialFeatureSelector(\n",
    "            model, \n",
    "            n_features_to_select=10,\n",
    "            direction=\"forward\", \n",
    "            scoring=rmse_scorer, \n",
    "            cv=cv,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        sfs.fit(X_train_scaled, y_train)\n",
    "        top_features = X_train_scaled.columns[sfs.get_support()].tolist()\n",
    "    \n",
    "    selected_features_dict[name] = top_features\n",
    "\n",
    "# Print selected features for each model\n",
    "for name, features in selected_features_dict.items():\n",
    "    print(f\"\\n{name} selected features ({len(features)}):\\n{features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean RMSE</th>\n",
       "      <th>Std RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest (selected features)</td>\n",
       "      <td>256861.690183</td>\n",
       "      <td>2619.875542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boosting (selected features)</td>\n",
       "      <td>259384.620203</td>\n",
       "      <td>2719.588572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagging (selected features)</td>\n",
       "      <td>267214.656144</td>\n",
       "      <td>3223.645995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression (selected features)</td>\n",
       "      <td>296258.306309</td>\n",
       "      <td>3098.653233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Regression (selected features)</td>\n",
       "      <td>296258.310320</td>\n",
       "      <td>3098.691796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression (selected features)</td>\n",
       "      <td>296258.312694</td>\n",
       "      <td>3098.687365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree (selected features)</td>\n",
       "      <td>309298.266696</td>\n",
       "      <td>4398.765961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Model      Mean RMSE     Std RMSE\n",
       "5      Random Forest (selected features)  256861.690183  2619.875542\n",
       "6  Gradient Boosting (selected features)  259384.620203  2719.588572\n",
       "4            Bagging (selected features)  267214.656144  3223.645995\n",
       "1   Ridge Regression (selected features)  296258.306309  3098.653233\n",
       "2   Lasso Regression (selected features)  296258.310320  3098.691796\n",
       "0  Linear Regression (selected features)  296258.312694  3098.687365\n",
       "3      Decision Tree (selected features)  309298.266696  4398.765961"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fs = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    selected_features = selected_features_dict[name]\n",
    "    X_selected = X_train_scaled[selected_features]\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        X_selected,\n",
    "        y_train,\n",
    "        scoring=rmse_scorer,\n",
    "        cv=cv\n",
    "    )\n",
    "\n",
    "    results_fs.append({\n",
    "        \"Model\": name + \" (selected features)\",\n",
    "        \"Mean RMSE\": -np.mean(scores),\n",
    "        \"Std RMSE\": np.std(scores)\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "results_fs_df = pd.DataFrame(results_fs).sort_values(\"Mean RMSE\")\n",
    "results_fs_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [2 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n",
    "\n",
    "- How did feature selection differ between linear and tree-based models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying feature selection, performance stayed fairly consistent for most models. Random Forest and Gradient Boosting still performed the best overall, suggesting they were already focusing on the most relevant features. However, reducing the feature set helped keep the models simpler while maintaining accuracy.\n",
    "\n",
    "Across models, features like `bathroomcnt`, `bedroomcnt`, and `calculatedfinishedsquarefeet` appeared frequently, indicating they were generally informative. Among the engineered features, `log_finishedsqft` and `sqft_to_lot_ratio` were selected by tree-based models, showing some value in those transformations.\n",
    "\n",
    "Tree-based models selected features based on importance scores and focused more on raw predictive power, while linear models relied on sequential selection and often picked more traditional housing metrics. This difference highlights how model structure affects which features are considered important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Top 3 Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far.\n",
    "\n",
    "1. Choose the top 3 models based on performance and interpretability from earlier parts.\n",
    "2. For each model:\n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, or other techniques from previous homeworks. \n",
    "   - Experiment with different versions of your feature engineering and preprocessing — treat these as additional tunable components.\n",
    "3. Report the mean and standard deviation of CV RMSE score for each model in a summary table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [50, 100, 150, 200],\n",
    "        \"max_depth\": [None, 10, 20, 30],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"n_estimators\": [50, 100, 150, 200],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"max_depth\": [3, 5]\n",
    "    },\n",
    "    \"Bagging\": {\n",
    "        \"n_estimators\": [25, 50, 100, 200, 300],\n",
    "        \"max_samples\": [0.5, 0.8, 0.95, 1.0]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Random Forest...\n",
      "Tuning Gradient Boosting...\n",
      "Tuning Bagging...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_results = []\n",
    "\n",
    "for name in [\"Random Forest\", \"Gradient Boosting\", \"Bagging\"]:\n",
    "    print(f\"Tuning {name}...\")\n",
    "\n",
    "    model = models[name]\n",
    "    features = selected_features_dict[name]\n",
    "    X_selected = X_train_scaled[features]\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[name],\n",
    "        scoring=rmse_scorer,\n",
    "        cv=cv,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_selected, y_train)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    best_score = -grid.best_score_\n",
    "    std_score = grid.cv_results_['std_test_score'][grid.best_index_]\n",
    "\n",
    "    tuned_results.append({\n",
    "        \"Model\": name + \" (tuned)\",\n",
    "        \"Mean RMSE\": best_score,\n",
    "        \"Std RMSE\": std_score,\n",
    "        \"Best Params\": grid.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean RMSE</th>\n",
       "      <th>Std RMSE</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting (tuned)</td>\n",
       "      <td>250263.274595</td>\n",
       "      <td>2591.358276</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (tuned)</td>\n",
       "      <td>251584.193858</td>\n",
       "      <td>2661.430878</td>\n",
       "      <td>{'max_depth': 20, 'max_features': 'sqrt', 'n_estimators': 200}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagging (tuned)</td>\n",
       "      <td>252752.723281</td>\n",
       "      <td>2917.594878</td>\n",
       "      <td>{'max_samples': 0.5, 'n_estimators': 300}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model      Mean RMSE     Std RMSE  \\\n",
       "1  Gradient Boosting (tuned)  250263.274595  2591.358276   \n",
       "0      Random Forest (tuned)  251584.193858  2661.430878   \n",
       "2            Bagging (tuned)  252752.723281  2917.594878   \n",
       "\n",
       "                                                      Best Params  \n",
       "1     {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}  \n",
       "0  {'max_depth': 20, 'max_features': 'sqrt', 'n_estimators': 200}  \n",
       "2                       {'max_samples': 0.5, 'n_estimators': 300}  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_results_df = pd.DataFrame(tuned_results).sort_values(\"Mean RMSE\")\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "tuned_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [4 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n",
    "- Provide a ranking of your three models and explain your reasoning — not just based on RMSE, but also interpretability, training time, or generalizability.\n",
    "- Conclude by considering whether this workflow has produced the results you expected. Typically, you would repeat steps 2 - 4 and also reconsider the choices you made in Milestone 1 when cleaning the dataset, until reaching the point of diminishing returns; do you think that would that have helped here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we fine-tuned our top-performing models (Random Forest, Gradient Boosting, and Bagging) using GridSearchCV. By defining lightweight parameter grids, we ensured that the search remained computationally efficient while still exploring key hyperparameters that affect performance.\n",
    "\n",
    "For **Random Forest**, we tuned parameters such as `n_estimators`, `max_depth`, and `max_features`. This allowed the model to strike a balance between bias and variance, and the grid search identified a combination that further reduced the RMSE compared to the default settings.\n",
    "\n",
    "For **Gradient Boosting**, we focused on `n_estimators`, `learning_rate`, and `max_depth`. The tuning process highlighted that a moderate learning rate with a controlled depth can capture complex relationships in the data without overfitting, leading to improved predictive performance.\n",
    "\n",
    "For the **Bagging** model, we tuned `n_estimators` and `max_samples` to optimize the resampling strategy and combine base learners more effectively. Although Bagging is less sensitive to hyperparameters compared to boosting methods, the grid search still provided valuable insights into the best configuration for our dataset.\n",
    "\n",
    "Overall, the tuned models showed lower and more stable RMSE values when compared to their untuned counterparts. This reinforces the importance of hyperparameter tuning as a practical way to enhance model performance while maintaining model simplicity and efficiency.\n",
    "\n",
    "Moreover, by limiting our grid search to computationally efficient intervals, we managed to reduce runtime significantly without compromising the quality of our results. This approach is particularly valuable in a production environment where speed and resource usage are critical.\n",
    "\n",
    "In summary, the fine-tuning process:\n",
    "- **Optimized key hyperparameters** for each model.\n",
    "- **Improved model performance**, as demonstrated by the RMSE scores.\n",
    "- **Maintained computational efficiency** through a focused parameter grid.\n",
    "\n",
    "We think that the success of this tuning strategy demonstrates that even a modest grid search can yield significant performance gains, making it a worthwhile step in the model development cycle.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
